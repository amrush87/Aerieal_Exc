{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np, sklearn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimp = Imputer(missing_values='NaN', strategy='median', axis=0)\\ndata_1 = imp.fit(data_1)\\ndata_2 = imp.fit(data_2)\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "#Reading data\n",
    "data_2013 = pd.read_csv('https://aerialintel.blob.core.windows.net/recruiting/datasets/wheat-2013-supervised.csv', delimiter=',')\n",
    "data_2014 = pd.read_csv('https://aerialintel.blob.core.windows.net/recruiting/datasets/wheat-2014-supervised.csv', delimiter=',')\n",
    "\n",
    "\"\"\"print(pd.isnull(data_1).any(1).nonzero()[0])\n",
    "print(pd.isnull(data_2).any(1).nonzero()[0])\n",
    "\"\"\"\n",
    "\n",
    "#Eliminating county and state columns\n",
    "data_1 = data_2013.iloc[:,2:]\n",
    "data_2 = data_2014.iloc[:,2:]\n",
    "\n",
    "#Eliminating Date column\n",
    "data_1 = data_1.drop('Date', axis=1)\n",
    "data_2 = data_2.drop('Date', axis=1)\n",
    "\n",
    "#Replacing Nan and empty cells with zeros\n",
    "data_1 = data_1.replace([np.inf, -np.inf], np.nan)\n",
    "data_2 = data_2.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "data_1 = data_1.fillna(value=0)\n",
    "data_2 = data_2.fillna(value=0)\n",
    "\n",
    "\n",
    "#sklearn.preprocessing.Imputer (impute through columns \"axis = 0\")\n",
    "#Not sure I want to use this, taking median/mean would be affected by county.\n",
    "#If I want to use this method, I have to consider each empty value to be median/mean of that particular county\n",
    "#filling empty spaces with zeros would be more neutral in this case even though not the best solution. \n",
    "\"\"\"\n",
    "imp = Imputer(missing_values='NaN', strategy='median', axis=0)\n",
    "data_1 = imp.fit(data_1)\n",
    "data_2 = imp.fit(data_2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((177493, 23), (182549, 23))\n",
      "((177493, 22), (182549, 22))\n",
      "((177493,), (182549,))\n"
     ]
    }
   ],
   "source": [
    "print(data_1.shape, data_2.shape)\n",
    "\n",
    "#separating Yield (Y1 and Y2) from Data (X1 and X2)\n",
    "X1 = data_1.iloc[:,0:-1]\n",
    "Y1 = data_1.iloc[:,-1]\n",
    "X2 = data_2.iloc[:,0:-1]\n",
    "Y2 = data_2.iloc[:,-1]\n",
    "\n",
    "print(X1.shape, X2.shape)\n",
    "print(Y1.shape, Y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.cross_validation import train_test_split\\nfrom sklearn.feature_selection import SelectKBest\\nfrom sklearn.feature_selection import f_regression\\n\\nX_1 = SelectKBest(f_regression, k=10).fit_transform(X1, np.reshape(Y1, (np.size(Y1),)))\\nX_2 = SelectKBest(f_regression, k=10).fit_transform(X2, np.reshape(Y2, (np.size(Y2),)))\\n\\nx_train1, x_test1, y_train1, y_test1 = train_test_split(X1,Y1)\\nx_train2, x_test2, y_train2, y_test2 = train_test_split(X2,Y2)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross-Validation method and selecting best features\n",
    "#I decided to go with another method (Lasso, below) because this would've been better with the regression method \n",
    "#I intended to use Stochastic Gradient Descent (SGDRegressor) first. However, that method gave horrible results. \n",
    "#The idea is that I thought that weather data being stochastic, a gradient descent would work. \n",
    "#I didn't explore it further. \n",
    "\n",
    "\"\"\"\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "X_1 = SelectKBest(f_regression, k=10).fit_transform(X1, np.reshape(Y1, (np.size(Y1),)))\n",
    "X_2 = SelectKBest(f_regression, k=10).fit_transform(X2, np.reshape(Y2, (np.size(Y2),)))\n",
    "\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(X1,Y1)\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(X2,Y2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Alternative method with better results. The idea is to create a n-fold cross-validated predictions. \n",
    "#I chose to build functions so it would be easier to test different sets of data. \n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def build_regr (data, labels):\n",
    "    #######\n",
    "    #Function that Builds classifier. \n",
    "    #Lasso regression should work well with the number of dimensions and amount of data at hand\n",
    "    #Choice is based on the flexibility of Lasso to select variables and regularize data\n",
    "    #INPUTS:\n",
    "    #data: training data\n",
    "    #labels: training labels\n",
    "    #####    \n",
    "    regr = Lasso()\n",
    "    regr.fit(data, labels)\n",
    "    return regr\n",
    "\n",
    "def classify (test_data, regr):\n",
    "    ######\n",
    "    #Function that gives out predictions of input test data given an input classifier\n",
    "    #INPUTS:\n",
    "    #test_data: data to be predicted given a classifier\n",
    "    #regr: Classifier object built in build_regr\n",
    "    #OUTPUTS:\n",
    "    #predictions of test_data\n",
    "    ######    \n",
    "    return regr.predict(test_data)\n",
    "\n",
    "def compare(predictions, test_labels,test_ind):\n",
    "    #####\n",
    "    #Calculating mean square error of predictions\n",
    "    #INPUTS:\n",
    "    #predictions:predicted values by the classifier\n",
    "    #test_labels: test data labels\n",
    "    #test_ind: the indices of the test data produced by KFold\n",
    "    #OUTPUTS:\n",
    "    #error: mean square error between test labels and predictions\n",
    "    ####\n",
    "\n",
    "    error = mean_squared_error((test_labels[test_ind]), (predictions))\n",
    "    return error\n",
    "\n",
    "def kf_regr (data,labels, n):\n",
    "    ######\n",
    "    #Function that divides the data to n-fold cross-validation, uses build_regr, classify, and compare\n",
    "    #INPUTS: \n",
    "    #data\n",
    "    #labels\n",
    "    #n: number of folds\n",
    "    #OUTPUTS:\n",
    "    #kf:the indices of training and test data in each fold\n",
    "    #pred_list: the list of predictions of each fold\n",
    "    #error_list: the mean square error of each fold\n",
    "    ########\n",
    "    \n",
    "    #KFold will create n sets of indices for n-fold training and test data\n",
    "    kf = KFold(len(data), n_folds = n)\n",
    "    itr = 0\n",
    "    for train_ind, test_ind in kf:\n",
    "        X_train, X_test = data.iloc[train_ind,:], data.iloc[test_ind,:] \n",
    "        y_train, y_test = labels.iloc[train_ind], labels.iloc[test_ind]\n",
    "        regr = build_regr(X_train, y_train)\n",
    "        pred = classify (X_test, regr)\n",
    "        error = compare (pred, y_test, test_ind)\n",
    "        if itr == 0:\n",
    "            pred_arr = pred\n",
    "            error_arr = error\n",
    "        else:\n",
    "            pred_arr = np.hstack((pred_arr, pred))\n",
    "            error_arr = np.vstack((error_arr, error))\n",
    "        print(itr)\n",
    "        itr += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    return kf, pred_arr, error_arr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "[ 46.52819997  47.48462402  47.80581869 ...,  42.7613864   42.56969583\n",
      "  42.43477013]\n",
      "[[ 182.98903296]\n",
      " [ 192.13064841]\n",
      " [ 184.53882192]\n",
      " [ 193.55930398]\n",
      " [ 183.05656075]\n",
      " [ 183.22135385]\n",
      " [ 181.13307127]\n",
      " [ 184.28285238]\n",
      " [ 174.92588128]\n",
      " [ 217.28942151]]\n",
      "((177493,), (10, 1))\n"
     ]
    }
   ],
   "source": [
    "#Cross-validated predictions for the wheat-2013-supervised data set\n",
    "\n",
    "kf1, pred1, error1 = kf_regr(X1, Y1, 10)\n",
    "mean_err1 = np.mean(error1)\n",
    "print(pred1)\n",
    "print(error1)\n",
    "\n",
    "print(pred1.shape, error1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "[ 43.19969458  44.24550964  43.27260885 ...,  37.61000543  37.59936708\n",
      "  37.42992835]\n",
      "[[ 86.15618833]\n",
      " [ 85.30782417]\n",
      " [ 85.14531109]\n",
      " [ 84.88176677]\n",
      " [ 84.59134188]\n",
      " [ 85.6385041 ]\n",
      " [ 84.70388938]\n",
      " [ 85.65137661]\n",
      " [ 85.53799909]\n",
      " [ 85.72346815]]\n",
      "((182549,), (10, 1))\n"
     ]
    }
   ],
   "source": [
    "#Cross-validated predictions for the wheat-2014-supervised data set\n",
    "\n",
    "kf2, pred2, error2 = kf_regr(X2, Y2, 10)\n",
    "mean_err2 = np.mean(error2)\n",
    "print(pred2)\n",
    "print(error2)\n",
    "\n",
    "print(pred2.shape, error2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((360042, 22), (360042,))\n"
     ]
    }
   ],
   "source": [
    "#Building a combined data set of wheat-2013-supervised and wheat-2014-supervised. \n",
    "\n",
    "XC = pd.concat((X1,X2), axis= 0, ignore_index = True)\n",
    "YC = pd.concat((Y1,Y2), axis= 0, ignore_index = True)\n",
    "\n",
    "print(XC.shape, YC.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "[ 46.34275771  46.76627244  46.98341879 ...,  39.46141393  39.45779223\n",
      "  39.24571937]\n",
      "[[ 194.23674345]\n",
      " [ 191.86664848]\n",
      " [ 186.49679127]\n",
      " [ 187.41588617]\n",
      " [ 184.21498667]\n",
      " [  89.36445685]\n",
      " [  87.96480064]\n",
      " [  91.07006899]\n",
      " [  87.6060564 ]\n",
      " [  86.46519984]]\n",
      "((360042,), (10, 1))\n"
     ]
    }
   ],
   "source": [
    "#Cross-validated predictions for the combined data set\n",
    "\n",
    "kfC, predC, errorC = kf_regr(XC, YC, 10)\n",
    "mean_errC = np.mean(errorC)\n",
    "print(predC)\n",
    "print(errorC)\n",
    "\n",
    "print(predC.shape, errorC.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((177493, 8), (182549, 8), (360042, 8))\n"
     ]
    }
   ],
   "source": [
    "#Appending results to to relevant data fields\n",
    "\n",
    "final1 = pd.DataFrame(data_2013.iloc[:,[0,1,2,3,4,-1]])\n",
    "final1['Predictions'] = pd.Series(pred1)\n",
    "final1['Mean Error'] = pd.Series(mean_err1)\n",
    "final2 = pd.DataFrame(data_2014.iloc[:,[0,1,2,3,4,-1]])\n",
    "final2['predictions'] = pd.Series(pred2)\n",
    "final2['Mean Error'] = pd.Series(mean_err2)\n",
    "finalC = pd.concat((data_2013.iloc[:,[0,1,2,3,4,-1]], data_2014.iloc[:,[0,1,2,3,4,-1]]), axis= 0, ignore_index= True)\n",
    "finalC['predictions'] = pd.Series(predC)\n",
    "finalC['Mean Error'] = pd.Series(mean_errC)\n",
    "\n",
    "print(final1.shape, final2.shape, finalC.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#writing results into .csv files\n",
    "\n",
    "final1.to_csv('predictions_2013.csv', sep=',')\n",
    "final2.to_csv('predictions_2014.csv', sep=',')\n",
    "finalC.to_csv('predictions_combined.csv', sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
